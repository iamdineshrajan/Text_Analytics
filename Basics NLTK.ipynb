{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcb93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c2df3",
   "metadata": {},
   "source": [
    "# Usage of PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd3de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339cd133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'write'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer=PorterStemmer()\n",
    "word_stemmer.stem('writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2febfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ad08e",
   "metadata": {},
   "source": [
    "# Usage of word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5b8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12676886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'obvious',\n",
       " 'question',\n",
       " 'that',\n",
       " 'came',\n",
       " 'in',\n",
       " 'our',\n",
       " 'mind',\n",
       " 'is',\n",
       " 'that',\n",
       " 'when',\n",
       " 'we',\n",
       " 'have',\n",
       " 'word',\n",
       " 'tokenizer',\n",
       " 'then',\n",
       " 'why',\n",
       " 'do',\n",
       " 'we',\n",
       " 'need',\n",
       " 'sentence',\n",
       " 'tokenizer',\n",
       " 'or',\n",
       " 'why',\n",
       " 'do',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'tokenize',\n",
       " 'text',\n",
       " 'into',\n",
       " 'sentences',\n",
       " '.',\n",
       " 'Suppose',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'count',\n",
       " 'average',\n",
       " 'words',\n",
       " 'in',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'how',\n",
       " 'we',\n",
       " 'can',\n",
       " 'do',\n",
       " 'this',\n",
       " '?',\n",
       " 'For',\n",
       " 'accomplishing',\n",
       " 'this',\n",
       " 'task',\n",
       " ',',\n",
       " 'we',\n",
       " 'need',\n",
       " 'both',\n",
       " 'sentence',\n",
       " 'tokenization',\n",
       " 'and',\n",
       " 'word',\n",
       " 'tokenization',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"\"\"An obvious question that came in our mind is that when we have word tokenizer then why do we need sentence tokenizer or why do we need to tokenize text into sentences.\n",
    "              Suppose we need to count average words in sentences, how we can do this? \n",
    "              For accomplishing this task, we need both sentence tokenization and word tokenization.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c988926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0804e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad664c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'obvious',\n",
       " 'question',\n",
       " 'that',\n",
       " 'came',\n",
       " 'in',\n",
       " 'our',\n",
       " 'mind',\n",
       " 'is',\n",
       " 'that',\n",
       " 'when',\n",
       " 'we',\n",
       " 'have',\n",
       " 'word',\n",
       " 'tokenizer',\n",
       " 'then',\n",
       " 'why',\n",
       " 'do',\n",
       " 'we',\n",
       " 'need',\n",
       " 'sentence',\n",
       " 'tokenizer',\n",
       " 'or',\n",
       " 'why',\n",
       " 'do',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'tokenize',\n",
       " 'text',\n",
       " 'into',\n",
       " 'sentences.',\n",
       " 'Suppose',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'count',\n",
       " 'average',\n",
       " 'words',\n",
       " 'in',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'how',\n",
       " 'we',\n",
       " 'can',\n",
       " 'do',\n",
       " 'this',\n",
       " '?',\n",
       " 'For',\n",
       " 'accomplishing',\n",
       " 'this',\n",
       " 'task',\n",
       " ',',\n",
       " 'we',\n",
       " 'need',\n",
       " 'both',\n",
       " 'sentence',\n",
       " 'tokenization',\n",
       " 'and',\n",
       " 'word',\n",
       " 'tokenization',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_tokenize=TreebankWordTokenizer()\n",
    "treebank_tokenize.tokenize(\"\"\"An obvious question that came in our mind is that when we have word tokenizer then why do we need sentence tokenizer or why do we need to tokenize text into sentences.\n",
    "              Suppose we need to count average words in sentences, how we can do this? \n",
    "              For accomplishing this task, we need both sentence tokenization and word tokenization.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ee1fc",
   "metadata": {},
   "source": [
    "# stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a14d11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a679151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_stop=stopwords.words('english')\n",
    "eng_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "becdb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"An obvious question that came in our mind is that when we have word tokenizer then why do we need sentence tokenizer or why do we need to tokenize text into sentences.\n",
    "              Suppose we need to count average words in sentences, how we can do this? \n",
    "              For accomplishing this task, we need both sentence tokenization and word tokenization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7593fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "token=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ccd5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'obvious',\n",
       " 'question',\n",
       " 'came',\n",
       " 'mind',\n",
       " 'word',\n",
       " 'tokenizer',\n",
       " 'need',\n",
       " 'sentence',\n",
       " 'tokenizer',\n",
       " 'need',\n",
       " 'tokenize',\n",
       " 'text',\n",
       " 'sentences',\n",
       " '.',\n",
       " 'Suppose',\n",
       " 'need',\n",
       " 'count',\n",
       " 'average',\n",
       " 'words',\n",
       " 'sentences',\n",
       " ',',\n",
       " '?',\n",
       " 'For',\n",
       " 'accomplishing',\n",
       " 'task',\n",
       " ',',\n",
       " 'need',\n",
       " 'sentence',\n",
       " 'tokenization',\n",
       " 'word',\n",
       " 'tokenization',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=[words for words in token if words not in eng_stop]\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1088a6",
   "metadata": {},
   "source": [
    "# wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a6f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15bfc86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"he's a tiger on the tennis court\", 'it aroused the tiger in me']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn=wn.synsets('tiger')[0]\n",
    "syn.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e3d7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiger.n.01'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfd9b34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a fierce or audacious person'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38736404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('person.n.01')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96203eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('abator.n.01'),\n",
       " Synset('abjurer.n.01'),\n",
       " Synset('abomination.n.01'),\n",
       " Synset('abstainer.n.02'),\n",
       " Synset('achiever.n.01'),\n",
       " Synset('acquaintance.n.03'),\n",
       " Synset('acquirer.n.01'),\n",
       " Synset('active.n.03'),\n",
       " Synset('actor.n.02'),\n",
       " Synset('adjudicator.n.01'),\n",
       " Synset('admirer.n.02'),\n",
       " Synset('adoptee.n.01'),\n",
       " Synset('adult.n.01'),\n",
       " Synset('adventurer.n.01'),\n",
       " Synset('adversary.n.01'),\n",
       " Synset('advisee.n.01'),\n",
       " Synset('advocate.n.01'),\n",
       " Synset('affiant.n.01'),\n",
       " Synset('african.n.01'),\n",
       " Synset('agnostic.n.01'),\n",
       " Synset('amateur.n.01'),\n",
       " Synset('amerindian.n.01'),\n",
       " Synset('ancient.n.02'),\n",
       " Synset('anomaly.n.02'),\n",
       " Synset('anti-american.n.01'),\n",
       " Synset('anti.n.01'),\n",
       " Synset('applicant.n.01'),\n",
       " Synset('appointee.n.02'),\n",
       " Synset('appreciator.n.01'),\n",
       " Synset('apprehender.n.02'),\n",
       " Synset('aquarius.n.01'),\n",
       " Synset('archaist.n.01'),\n",
       " Synset('aries.n.01'),\n",
       " Synset('arrogator.n.01'),\n",
       " Synset('assessee.n.01'),\n",
       " Synset('asthmatic.n.01'),\n",
       " Synset('authority.n.02'),\n",
       " Synset('autodidact.n.01'),\n",
       " Synset('baby_boomer.n.01'),\n",
       " Synset('baby_buster.n.01'),\n",
       " Synset('bad_guy.n.01'),\n",
       " Synset('bad_person.n.01'),\n",
       " Synset('baldhead.n.01'),\n",
       " Synset('balker.n.01'),\n",
       " Synset('bather.n.02'),\n",
       " Synset('beard.n.03'),\n",
       " Synset('bedfellow.n.02'),\n",
       " Synset('bereaved.n.01'),\n",
       " Synset('best.n.02'),\n",
       " Synset('birth.n.05'),\n",
       " Synset('biter.n.01'),\n",
       " Synset('black.n.05'),\n",
       " Synset('blogger.n.01'),\n",
       " Synset('blond.n.01'),\n",
       " Synset('bluecoat.n.01'),\n",
       " Synset('bodybuilder.n.01'),\n",
       " Synset('bomber.n.02'),\n",
       " Synset('brunet.n.01'),\n",
       " Synset('bullfighter.n.01'),\n",
       " Synset('buster.n.04'),\n",
       " Synset('cancer.n.02'),\n",
       " Synset('candidate.n.02'),\n",
       " Synset('capitalist.n.02'),\n",
       " Synset('capricorn.n.01'),\n",
       " Synset('captor.n.01'),\n",
       " Synset('case.n.06'),\n",
       " Synset('cashier.n.02'),\n",
       " Synset('celebrant.n.01'),\n",
       " Synset('censor.n.01'),\n",
       " Synset('chameleon.n.01'),\n",
       " Synset('changer.n.01'),\n",
       " Synset('charmer.n.02'),\n",
       " Synset('child.n.03'),\n",
       " Synset('chutzpanik.n.01'),\n",
       " Synset('closer.n.01'),\n",
       " Synset('clumsy_person.n.01'),\n",
       " Synset('collector.n.01'),\n",
       " Synset('color-blind_person.n.01'),\n",
       " Synset('combatant.n.01'),\n",
       " Synset('commoner.n.01'),\n",
       " Synset('communicator.n.01'),\n",
       " Synset('complexifier.n.01'),\n",
       " Synset('compulsive.n.01'),\n",
       " Synset('computer_user.n.01'),\n",
       " Synset('contemplative.n.01'),\n",
       " Synset('contestant.n.01'),\n",
       " Synset('convert.n.01'),\n",
       " Synset('copycat.n.01'),\n",
       " Synset('counter.n.05'),\n",
       " Synset('counterterrorist.n.01'),\n",
       " Synset('coward.n.01'),\n",
       " Synset('crawler.n.02'),\n",
       " Synset('creator.n.02'),\n",
       " Synset('creature.n.02'),\n",
       " Synset('creditor.n.01'),\n",
       " Synset('cripple.n.01'),\n",
       " Synset('dancer.n.02'),\n",
       " Synset('dead_person.n.01'),\n",
       " Synset('deaf_person.n.01'),\n",
       " Synset('debaser.n.01'),\n",
       " Synset('debtor.n.01'),\n",
       " Synset('defecator.n.01'),\n",
       " Synset('delayer.n.01'),\n",
       " Synset('deliverer.n.04'),\n",
       " Synset('demander.n.01'),\n",
       " Synset('dieter.n.01'),\n",
       " Synset('differentiator.n.01'),\n",
       " Synset('disentangler.n.01'),\n",
       " Synset('disputant.n.01'),\n",
       " Synset('dissenter.n.01'),\n",
       " Synset('divider.n.02'),\n",
       " Synset('domestic_partner.n.01'),\n",
       " Synset('double.n.03'),\n",
       " Synset('dresser.n.02'),\n",
       " Synset('dribbler.n.02'),\n",
       " Synset('drug_user.n.01'),\n",
       " Synset('dyslectic.n.01'),\n",
       " Synset('ectomorph.n.01'),\n",
       " Synset('effecter.n.01'),\n",
       " Synset('elizabethan.n.01'),\n",
       " Synset('emotional_person.n.01'),\n",
       " Synset('endomorph.n.01'),\n",
       " Synset('engineer.n.01'),\n",
       " Synset('enjoyer.n.01'),\n",
       " Synset('enrollee.n.01'),\n",
       " Synset('entertainer.n.01'),\n",
       " Synset('ethnic.n.01'),\n",
       " Synset('experimenter.n.02'),\n",
       " Synset('expert.n.01'),\n",
       " Synset('explorer.n.01'),\n",
       " Synset('extrovert.n.01'),\n",
       " Synset('face.n.05'),\n",
       " Synset('faddist.n.01'),\n",
       " Synset('faller.n.02'),\n",
       " Synset('fastener.n.01'),\n",
       " Synset('female.n.02'),\n",
       " Synset('fiduciary.n.01'),\n",
       " Synset('first-rater.n.01'),\n",
       " Synset('follower.n.01'),\n",
       " Synset('free_agent.n.02'),\n",
       " Synset('friend.n.01'),\n",
       " Synset('fugitive.n.01'),\n",
       " Synset('gainer.n.01'),\n",
       " Synset('gainer.n.02'),\n",
       " Synset('gambler.n.01'),\n",
       " Synset('gatekeeper.n.01'),\n",
       " Synset('gatherer.n.01'),\n",
       " Synset('gemini.n.01'),\n",
       " Synset('gentile.n.02'),\n",
       " Synset('good_guy.n.01'),\n",
       " Synset('good_person.n.01'),\n",
       " Synset('granter.n.01'),\n",
       " Synset('greeter.n.01'),\n",
       " Synset('grinner.n.01'),\n",
       " Synset('groaner.n.01'),\n",
       " Synset('grunter.n.01'),\n",
       " Synset('guesser.n.01'),\n",
       " Synset('handicapped_person.n.01'),\n",
       " Synset('hater.n.01'),\n",
       " Synset('heterosexual.n.01'),\n",
       " Synset('homosexual.n.01'),\n",
       " Synset('homunculus.n.02'),\n",
       " Synset('hope.n.04'),\n",
       " Synset('hoper.n.01'),\n",
       " Synset('huddler.n.02'),\n",
       " Synset('hugger.n.01'),\n",
       " Synset('immune.n.01'),\n",
       " Synset('individualist.n.01'),\n",
       " Synset('inhabitant.n.01'),\n",
       " Synset('innocent.n.01'),\n",
       " Synset('insured.n.01'),\n",
       " Synset('intellectual.n.01'),\n",
       " Synset('interpreter.n.02'),\n",
       " Synset('introvert.n.01'),\n",
       " Synset('jat.n.01'),\n",
       " Synset('jew.n.01'),\n",
       " Synset('jewel.n.02'),\n",
       " Synset('jumper.n.01'),\n",
       " Synset('junior.n.03'),\n",
       " Synset('juvenile.n.01'),\n",
       " Synset('killer.n.01'),\n",
       " Synset('kink.n.03'),\n",
       " Synset('kneeler.n.01'),\n",
       " Synset('knocker.n.02'),\n",
       " Synset('knower.n.01'),\n",
       " Synset('large_person.n.01'),\n",
       " Synset('latin.n.03'),\n",
       " Synset('laugher.n.01'),\n",
       " Synset('leader.n.01'),\n",
       " Synset('learner.n.01'),\n",
       " Synset('left-hander.n.02'),\n",
       " Synset('leo.n.01'),\n",
       " Synset('libra.n.01'),\n",
       " Synset('life.n.08'),\n",
       " Synset('lightning_rod.n.01'),\n",
       " Synset('linguist.n.02'),\n",
       " Synset('literate.n.01'),\n",
       " Synset('liver.n.03'),\n",
       " Synset('longer.n.01'),\n",
       " Synset('loose_cannon.n.01'),\n",
       " Synset('loved_one.n.01'),\n",
       " Synset('lover.n.01'),\n",
       " Synset('machine.n.02'),\n",
       " Synset('mailer.n.02'),\n",
       " Synset('malcontent.n.01'),\n",
       " Synset('male.n.02'),\n",
       " Synset('man.n.03'),\n",
       " Synset('man_jack.n.01'),\n",
       " Synset('manipulator.n.02'),\n",
       " Synset('married.n.01'),\n",
       " Synset('masturbator.n.01'),\n",
       " Synset('measurer.n.01'),\n",
       " Synset('mesomorph.n.01'),\n",
       " Synset('mestizo.n.01'),\n",
       " Synset('middlebrow.n.01'),\n",
       " Synset('miracle_man.n.01'),\n",
       " Synset('misogamist.n.01'),\n",
       " Synset('mixed-blood.n.01'),\n",
       " Synset('modern.n.01'),\n",
       " Synset('money_handler.n.01'),\n",
       " Synset('monolingual.n.01'),\n",
       " Synset('mother_hen.n.01'),\n",
       " Synset('mouse.n.03'),\n",
       " Synset('mutilator.n.01'),\n",
       " Synset('namer.n.01'),\n",
       " Synset('namesake.n.01'),\n",
       " Synset('national.n.01'),\n",
       " Synset('native.n.01'),\n",
       " Synset('native.n.02'),\n",
       " Synset('neglecter.n.01'),\n",
       " Synset('neighbor.n.01'),\n",
       " Synset('neutral.n.01'),\n",
       " Synset('nondescript.n.01'),\n",
       " Synset('nonmember.n.01'),\n",
       " Synset('nonparticipant.n.01'),\n",
       " Synset('nonpartisan.n.01'),\n",
       " Synset('nonperson.n.01'),\n",
       " Synset('nonreligious_person.n.01'),\n",
       " Synset('nonresident.n.01'),\n",
       " Synset('nonsmoker.n.01'),\n",
       " Synset('nonworker.n.01'),\n",
       " Synset('nude.n.03'),\n",
       " Synset('nurser.n.01'),\n",
       " Synset('occultist.n.01'),\n",
       " Synset('optimist.n.01'),\n",
       " Synset('orphan.n.02'),\n",
       " Synset('ostrich.n.01'),\n",
       " Synset('ouster.n.01'),\n",
       " Synset('outcaste.n.01'),\n",
       " Synset('outdoorsman.n.01'),\n",
       " Synset('owner.n.02'),\n",
       " Synset('pamperer.n.01'),\n",
       " Synset('pansexual.n.01'),\n",
       " Synset('pardoner.n.01'),\n",
       " Synset('partner.n.03'),\n",
       " Synset('party.n.05'),\n",
       " Synset('passer.n.02'),\n",
       " Synset('peer.n.01'),\n",
       " Synset('perceiver.n.01'),\n",
       " Synset('percher.n.01'),\n",
       " Synset('person_of_color.n.01'),\n",
       " Synset('personage.n.01'),\n",
       " Synset('personification.n.01'),\n",
       " Synset('perspirer.n.01'),\n",
       " Synset('philosopher.n.02'),\n",
       " Synset('picker.n.01'),\n",
       " Synset('pisces.n.02'),\n",
       " Synset('pisser.n.01'),\n",
       " Synset('planner.n.01'),\n",
       " Synset('player.n.04'),\n",
       " Synset('posturer.n.01'),\n",
       " Synset('powderer.n.01'),\n",
       " Synset('precursor.n.02'),\n",
       " Synset('preserver.n.03'),\n",
       " Synset('primitive.n.01'),\n",
       " Synset('propositus.n.01'),\n",
       " Synset('public_relations_person.n.01'),\n",
       " Synset('pursuer.n.02'),\n",
       " Synset('pussycat.n.01'),\n",
       " Synset('quarter.n.11'),\n",
       " Synset('quitter.n.01'),\n",
       " Synset('radical.n.03'),\n",
       " Synset('realist.n.02'),\n",
       " Synset('rectifier.n.02'),\n",
       " Synset('redhead.n.01'),\n",
       " Synset('registrant.n.01'),\n",
       " Synset('relative.n.01'),\n",
       " Synset('reliever.n.02'),\n",
       " Synset('religious_person.n.01'),\n",
       " Synset('repeater.n.01'),\n",
       " Synset('rescuer.n.02'),\n",
       " Synset('rester.n.01'),\n",
       " Synset('restrainer.n.02'),\n",
       " Synset('revenant.n.01'),\n",
       " Synset('rich_person.n.01'),\n",
       " Synset('right-hander.n.02'),\n",
       " Synset('riser.n.01'),\n",
       " Synset('romper.n.01'),\n",
       " Synset('roundhead.n.01'),\n",
       " Synset('ruler.n.01'),\n",
       " Synset('rusher.n.03'),\n",
       " Synset('sagittarius.n.01'),\n",
       " Synset('scientist.n.01'),\n",
       " Synset('scorpio.n.01'),\n",
       " Synset('scratcher.n.02'),\n",
       " Synset('second-rater.n.01'),\n",
       " Synset('seeder.n.01'),\n",
       " Synset('seeker.n.01'),\n",
       " Synset('segregate.n.01'),\n",
       " Synset('self.n.02'),\n",
       " Synset('sensualist.n.01'),\n",
       " Synset('sentimentalist.n.01'),\n",
       " Synset('sex_object.n.01'),\n",
       " Synset('sex_symbol.n.01'),\n",
       " Synset('shaker.n.01'),\n",
       " Synset('showman.n.01'),\n",
       " Synset('signer.n.02'),\n",
       " Synset('simpleton.n.01'),\n",
       " Synset('six-footer.n.01'),\n",
       " Synset('skidder.n.01'),\n",
       " Synset('slav.n.01'),\n",
       " Synset('slave.n.01'),\n",
       " Synset('slave.n.03'),\n",
       " Synset('sleepyhead.n.01'),\n",
       " Synset('sloucher.n.01'),\n",
       " Synset('small_person.n.01'),\n",
       " Synset('smasher.n.01'),\n",
       " Synset('smiler.n.01'),\n",
       " Synset('sneezer.n.01'),\n",
       " Synset('sniffer.n.01'),\n",
       " Synset('sniffler.n.01'),\n",
       " Synset('snuffer.n.02'),\n",
       " Synset('snuffler.n.01'),\n",
       " Synset('socializer.n.01'),\n",
       " Synset('sort.n.03'),\n",
       " Synset('sounding_board.n.01'),\n",
       " Synset('sphinx.n.01'),\n",
       " Synset('spitter.n.01'),\n",
       " Synset('sport.n.04'),\n",
       " Synset('sprawler.n.01'),\n",
       " Synset('spurner.n.01'),\n",
       " Synset('squinter.n.01'),\n",
       " Synset('stifler.n.01'),\n",
       " Synset('stigmatic.n.01'),\n",
       " Synset('stooper.n.02'),\n",
       " Synset('stranger.n.02'),\n",
       " Synset('struggler.n.01'),\n",
       " Synset('subject.n.06'),\n",
       " Synset('supernumerary.n.01'),\n",
       " Synset('surrenderer.n.01'),\n",
       " Synset('survivalist.n.01'),\n",
       " Synset('survivor.n.02'),\n",
       " Synset('suspect.n.01'),\n",
       " Synset('tagger.n.01'),\n",
       " Synset('tagger.n.02'),\n",
       " Synset('tapper.n.02'),\n",
       " Synset('taurus.n.02'),\n",
       " Synset('tempter.n.01'),\n",
       " Synset('termer.n.01'),\n",
       " Synset('terror.n.02'),\n",
       " Synset('testator.n.01'),\n",
       " Synset('thin_person.n.01'),\n",
       " Synset('third-rater.n.01'),\n",
       " Synset('thrower.n.02'),\n",
       " Synset('tiger.n.01'),\n",
       " Synset('totemist.n.01'),\n",
       " Synset('toucher.n.01'),\n",
       " Synset('transfer.n.02'),\n",
       " Synset('transsexual.n.02'),\n",
       " Synset('transvestite.n.01'),\n",
       " Synset('traveler.n.01'),\n",
       " Synset('trier.n.02'),\n",
       " Synset('turner.n.07'),\n",
       " Synset('tyrant.n.03'),\n",
       " Synset('undoer.n.02'),\n",
       " Synset('unfortunate.n.01'),\n",
       " Synset('unskilled_person.n.01'),\n",
       " Synset('unwelcome_person.n.01'),\n",
       " Synset('user.n.01'),\n",
       " Synset('vanisher.n.01'),\n",
       " Synset('victim.n.02'),\n",
       " Synset('victorian.n.01'),\n",
       " Synset('virgo.n.01'),\n",
       " Synset('visionary.n.01'),\n",
       " Synset('visually_impaired_person.n.01'),\n",
       " Synset('waiter.n.02'),\n",
       " Synset('waker.n.02'),\n",
       " Synset('walk-in.n.01'),\n",
       " Synset('wanter.n.01'),\n",
       " Synset('ward.n.01'),\n",
       " Synset('warrior.n.01'),\n",
       " Synset('watcher.n.03'),\n",
       " Synset('weakling.n.01'),\n",
       " Synset('weasel.n.01'),\n",
       " Synset('white.n.01'),\n",
       " Synset('wiggler.n.01'),\n",
       " Synset('winker.n.01'),\n",
       " Synset('withholder.n.01'),\n",
       " Synset('witness.n.05'),\n",
       " Synset('worker.n.01'),\n",
       " Synset('worldling.n.01'),\n",
       " Synset('yawner.n.01')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2c3b7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d295560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('tiger.n.01.tiger')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64437dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1682abaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an obvious question that came in our mind is that when we have word tokenizer then why do we need sentence tokenizer or why do we need to tokenize text into sentences.\\n              suppose we need to count average words in sentences, how we can do this? \\n              for accomplishing this task, we need both sentence tokenization and word tokenization.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lanc_stem=LancasterStemmer()\n",
    "lanc_stem.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45f79a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ce7c69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An obvious quest that came in our mind is that when we have word tokenizer then why do we need sentence tokenizer or why do we need to tokenize text into sentences.\\n              Suppose we need to count average words in sentences, how we can do this? \\n              For accomplishing this task, we need both sentence tokenizat and word tokenizat.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp=RegexpStemmer('ion')\n",
    "regexp.stem(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f9461",
   "metadata": {},
   "source": [
    "# SnowballStemmer is used to stem other non english languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce1593",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e30d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ce3941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "lemma=lemmatizer.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42315e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An obvious question that came in our mind is that when we have word tokenizer then why do we need sentence tokenizer or why do we need to tokenize text into sentences.\\n              Suppose we need to count average words in sentences, how we can do this? \\n              For accomplishing this task, we need both sentence tokenization and word tokenization.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302cdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
